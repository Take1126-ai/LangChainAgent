
import sys
from pathlib import Path
import pytest
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage

# Add project root to sys.path to allow importing from src
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.agent import run_agent, AgentState
from src.config import Config

# Mock LLM and other external dependencies to isolate the logic
class MockLLM:
    def invoke(self, *args, **kwargs):
        return AIMessage(content="Mocked LLM response")

    def bind_tools(self, *args, **kwargs):
        return self

# Monkeypatch the LLM used in the agent module
sys.modules['src.core.agent'].llm = MockLLM()
sys.modules['src.core.agent'].llm_with_tools = MockLLM()


def run_summarization_logic(chat_history, max_turns, summary_turns):
    """
    A helper function to isolate and test the summarization logic from run_agent.
    It simulates the relevant parts of the run_agent function.
    """
    # Set mock config values
    Config.MAX_CONVERSATION_TURNS = max_turns
    Config.SUMMARY_CONVERSATION_TURNS = summary_turns
    Config.DEBUG_MODE = False
    Config.WRITE_INNER_THOUGHTS = False

    # Create a minimal state
    state = AgentState(
        input="test input",
        chat_history=chat_history,
        always_allowed_tools=set(),
    )

    # This is the new AIMessage that would be generated by the LLM in a real run
    result_message = AIMessage(content="current llm response")

    # --- Start of the logic copied and adapted from run_agent ---
    if Config.MAX_CONVERSATION_TURNS > 0 and len(state["chat_history"]) >= Config.MAX_CONVERSATION_TURNS:
        split_point = len(state["chat_history"]) - Config.SUMMARY_CONVERSATION_TURNS
        
        if split_point > 0:
            while split_point > 0 and isinstance(state["chat_history"][split_point], ToolMessage):
                split_point -= 1
        
        split_point = max(0, split_point)
        
        messages_to_summarize = state["chat_history"][:split_point]
        recent_history = state["chat_history"][split_point:]

        if not messages_to_summarize:
            # In the real agent, this would return. For the test, we'll return the lists.
            new_chat_history = state["chat_history"] + [result_message]
            return messages_to_summarize, recent_history, new_chat_history

        # Simulate summarization
        summary_message = AIMessage(content="会話の要約: Mocked summary")
        
        new_chat_history = [summary_message] + recent_history
        
        # In the real agent, the new result is added after this block.
        # We add it here to match the final return value.
        return messages_to_summarize, recent_history, new_chat_history + [result_message]
    
    # --- End of copied logic ---

    # If no summarization happened
    new_chat_history = state["chat_history"] + [result_message]
    return [], state["chat_history"], new_chat_history


def test_summary_boundary_adjustment_preserves_tool_calls():
    """
    Tests that the summarization logic correctly adjusts the boundary
    to keep an AIMessage with tool_calls and its corresponding ToolMessages together.
    """
    ai_tool_call = AIMessage(
        content="",
        tool_calls=[{"name": "test_tool", "args": {}, "id": "tool_123"}]
    )
    tool_response = ToolMessage(content="tool result", tool_call_id="tool_123")

    chat_history = [
        HumanMessage(content="Step 1"),
        AIMessage(content="Okay."),
        HumanMessage(content="Step 2: Please use the tool."),
        ai_tool_call,  # This is at index 3
        tool_response, # This is at index 4
    ]
    
    # History length = 5
    # MAX_TURNS = 4 (triggers summarization)
    # SUMMARY_TURNS = 2
    #
    # Old logic would split at index 5 - 2 = 3.
    # recent_history would be [ai_tool_call, tool_response]. This is wrong.
    # It should be [ai_tool_call, tool_response].
    # The messages to summarize would be chat_history[:-2] = [HM, AM, HM]
    # The recent history would be chat_history[-2:] = [ai_tool_call, tool_response]
    # Let's make it break. SUMMARY_TURNS = 1
    # Old logic split point = 5 - 1 = 4.
    # messages_to_summarize = chat_history[:4] = [HM, AM, HM, ai_tool_call]
    # recent_history = chat_history[4:] = [tool_response] -> BROKEN!

    max_turns = 4
    summary_turns = 1

    messages_to_summarize, recent_history, _ = run_summarization_logic(chat_history, max_turns, summary_turns)

    # New logic should adjust split_point from 4 down to 3.
    # Expected messages_to_summarize = chat_history[:3]
    # Expected recent_history = chat_history[3:]

    assert len(messages_to_summarize) == 3
    assert messages_to_summarize[0].content == "Step 1"
    assert messages_to_summarize[2].content == "Step 2: Please use the tool."

    assert len(recent_history) == 2
    assert recent_history[0] is ai_tool_call
    assert recent_history[1] is tool_response


def test_summary_no_adjustment_needed():
    """
    Tests that the logic doesn't adjust the boundary when it's not necessary.
    """
    chat_history = [
        HumanMessage(content="1"), AIMessage(content="2"),
        HumanMessage(content="3"), AIMessage(content="4"),
        HumanMessage(content="5"), AIMessage(content="6"),
    ]
    max_turns = 5
    summary_turns = 2

    messages_to_summarize, recent_history, _ = run_summarization_logic(chat_history, max_turns, summary_turns)

    # Split point should be 6 - 2 = 4. No ToolMessage at index 4.
    # No adjustment should happen.
    assert len(messages_to_summarize) == 4
    assert len(recent_history) == 2
    assert recent_history[0].content == "5"
    assert recent_history[1].content == "6"

def test_summary_with_multiple_tool_messages():
    """
    Tests that the logic correctly includes a sequence of multiple ToolMessages.
    """
    ai_tool_call = AIMessage(
        content="",
        tool_calls=[
            {"name": "tool_a", "args": {}, "id": "A"},
            {"name": "tool_b", "args": {}, "id": "B"},
        ]
    )
    
    chat_history = [
        HumanMessage(content="pre-context"),
        ai_tool_call,  # index 1
        ToolMessage(content="res_A", tool_call_id="A"), # index 2
        ToolMessage(content="res_B", tool_call_id="B"), # index 3
    ]

    # History length = 4.
    # MAX_TURNS = 3, SUMMARY_TURNS = 2
    # Old logic split point = 4 - 2 = 2.
    # recent_history = [ToolMessage A, ToolMessage B] -> BROKEN
    
    max_turns = 3
    summary_turns = 2

    messages_to_summarize, recent_history, _ = run_summarization_logic(chat_history, max_turns, summary_turns)

    # New logic should adjust split_point from 2 down to 1.
    # Expected messages_to_summarize = chat_history[:1]
    # Expected recent_history = chat_history[1:]
    assert len(messages_to_summarize) == 1
    assert messages_to_summarize[0].content == "pre-context"

    assert len(recent_history) == 3
    assert recent_history[0] is ai_tool_call
    assert recent_history[1].content == "res_A"
    assert recent_history[2].content == "res_B"
